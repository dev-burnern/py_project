import pandas as pd
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import CountVectorizer

try:
    kiwi = Kiwi()
except Exception as e:
    print("[ì˜¤ë¥˜] Kiwi ë¶ˆëŸ¬ì˜¤ëŠ” ë° ë¬¸ì œê°€ ìˆì–´ìš”. pip install kiwipiepy í™•ì¸í•´ì£¼ì„¸ìš”.", e)
    kiwi = None

STOPWORDS = {
    "ã…‹ã…‹", "ã…ã…", "ã… ã… ", "ì´ê±°", "ì €ê±°", "ê·¸ê±°", "ê·¼ë°",
    "ì§„ì§œ", "ë„ˆë¬´", "ì•„ë‹ˆ", "ì´ì œ", "ì˜¤ëŠ˜", "ë‚´ì¼", "ê·¸ëƒ¥",
    "ì‚¬ëŒ", "ìƒê°", "ì¢€", "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì‚¬ì§„"
}

def analyze_participation(df):
    if df is None or df.empty:
        return []

    counts = df["sender"].value_counts()
    total = len(df)

    result = []
    for sender, cnt in counts.items():
        info = {
            "sender": sender,
            "count": int(cnt),
            "ratio": round(cnt / total * 100, 1)
        }
        result.append(info)

    return result


def extract_keywords(df, top_n=20):
    if df is None or df.empty:
        return []
    if kiwi is None:
        return []

    messages_series = df["message"].dropna().astype(str)
    messages = messages_series.tolist()

    if len(messages) == 0:
        return []

    def kiwi_tokenizer(text):
        tokens = kiwi.tokenize(text)
        words = []
        for t in tokens:
            if t.tag.startswith("NN") and len(t.form) > 1 and t.form not in STOPWORDS:
                words.append(t.form)
        return words

    vectorizer = CountVectorizer(
        tokenizer=kiwi_tokenizer,
        max_features=top_n
    )

    try:
        X = vectorizer.fit_transform(messages)
        feature_names = vectorizer.get_feature_names_out()

        sums = X.sum(axis=0)
        try:
            word_counts = sums.A1
        except Exception:
            word_counts = sums.tolist()[0]

    except ValueError:
        return []
    except Exception as e:
        print("[í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜]", e)
        return []

    keywords = []
    for word, count in zip(feature_names, word_counts):
        keywords.append({
            "word": word,
            "count": int(count)
        })

    keywords.sort(key=lambda x: x["count"], reverse=True)
    return keywords


def analyze_time_distribution(df):
    if df is None or df.empty:
        return []

    dt_series = None

    if "datetime" in df.columns:
        dt_series = pd.to_datetime(df["datetime"], errors="coerce")
    elif "time" in df.columns:
        dt_series = pd.to_datetime(df["time"], format="%H:%M", errors="coerce")
    else:
        return []

    dt_series = dt_series.dropna()
    if dt_series.empty:
        return []

    counts = dt_series.dt.hour.value_counts().sort_index()

    result = []
    for hour in range(24):
        cnt = counts.get(hour, 0)
        result.append({
            "hour": int(hour),
            "count": int(cnt)
        })

    return result


def infer_love_insight(keywords):
    if not keywords:
        return {
            "interestScore": 0,
            "interestLabel": "ë°ì´í„° ë¶€ì¡± ğŸ˜¢",
            "topic": "ëŒ€í™”ëŸ‰ì´ ë„ˆë¬´ ì ì–´ì„œ ë§ˆìŒì„ ì½ê¸° ì–´ë ¤ì›Œìš”.",
            "summary": "ì¡°ê¸ˆ ë” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ë‹¤ì‹œ ë¶„ì„í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
        }

    words_set = set()
    for k in keywords:
        word = k.get("word")
        if word:
            words_set.add(word)

    strong_love_words = {
        "ì‚¬ë‘", "ì‚¬ë‘í•´", "ì¢‹ì•„í•´", "ë„ˆë°–ì—", "ë³´ê³ ì‹¶", "ë³´ê³ ì‹¶ë‹¤",
        "ë³´ê³  ì‹¶ë‹¤", "ì„¤ë ˆ", "ì¸", "ì‹¬ì¿µ", "ê³ ë°±", "ì‚¬ê·€ì", "ì—°ì• ","ìê¸°", "ìê¸°ì•¼", "ì—¬ë³´", "ì—¬ë³´ì•¼"
        "ê³µì£¼","ì™•ì","ë‚´êº¼","ë‚´êº¼ì•¼","ê²°í˜¼","ìˆ ","í¬ë¦¬ìŠ¤ë§ˆìŠ¤"
    }
    light_love_words = {
        "ê·€ì—½", "ê·€ì—¬ì›Œ", "ì´ì˜ë‹¤", "ì˜ˆì˜ë‹¤", "ì˜ìƒê²¼", "ë©‹ìˆë‹¤",
        "ë°ì´íŠ¸", "ì˜í™”", "ë°¥ë¨¹ì", "ë°¥ì´ë‚˜", "ìˆ í•œì”", "ìˆ  í•œì”",
        "ë§Œë‚ ê¹Œ", "ë³´ì", "ë§Œë‚˜ì","ì—°ë½", "ì „í™”", "ì‹¬ì‹¬", "ë³´ê³ ì‹¶ë„¤", "ë§Œë‚˜",
        "ì–¸ì œ", "ì‹œê°„", "ì•½ì†"
    }
    cold_words = {
        "ë°”ë¹ ", "í”¼ê³¤", "ë‚˜ì¤‘ì—", "ê·€ì°®", "í˜ë“¤", "ê´€ì‹¬ì—†", "ëì–´",
        "ê·¸ë§Œ", "ëª°ë¼", "ì‹«ì–´", "ì•ˆë¼"
    }

    strong_hits = sum(1 for w in strong_love_words if w in words_set)
    light_hits = sum(1 for w in light_love_words if w in words_set)
    cold_hits = sum(1 for w in cold_words if w in words_set)

    score = 50
    score += 18 * strong_hits
    score += 10 * light_hits
    score -= 25 * cold_hits

    if score < 0:
        score = 0
    if score > 100:
        score = 100

    if score >= 80:
        label = "ì € ëª°ë˜ ë‘ë¶„ ì´ë¯¸ ì‚¬ê·€ê³  ìˆì£ ? ğŸ’˜"
        topic = "ìƒë‹¹íˆ ë‹¬ë‹¬í•œ ë¶„ìœ„ê¸°! ì„œë¡œ ë§ˆìŒì´ í†µí•˜ëŠ” ëŠë‚Œì´ì—ìš” ë‚¨ì€ ê±´ ê³ ë°± ë¿!."
        summary = (
            "ëŒ€í™”ì— ì• ì • í‘œí˜„ì´ë‚˜ ë§Œë‚¨ ì œì•ˆ, ì„¤ë ˆëŠ” ë‰˜ì•™ìŠ¤ê°€ ë§ì´ ë³´ì—¬ìš”. "
            "ìƒëŒ€ë°©ì´ ë‹¹ì‹ ì—ê²Œ ê½¤ ë§ì€ í˜¸ê°ì„ ê°€ì§€ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ìš”!"
        )
    elif score >= 60:
        label = "í˜¸ê° ìˆëŠ” í¸ ğŸ’—"
        topic = "ì¹œê·¼í•¨ ì†ì— ë¬˜í•œ ì„¤ë ˜ì´ ëŠê»´ì§€ëŠ” ëŒ€í™”ë„¤ìš”."
        summary = (
            "ì¹œê·¼í•œ ë†ë‹´ê³¼ ê°€ë²¼ìš´ ì• ì • í‘œí˜„, ë§Œë‚¨ ì´ì•¼ê¸°ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì˜¤ê°€ëŠ” í¸ì´ì—ìš”. "
            "ì„œë¡œ ëˆˆì¹˜ë¥¼ ë³´ëŠ” ë‹¨ê³„ì¼ ìˆ˜ ìˆê³ , ì´ì œë¶€í„° ì‘ì€ ë””í…Œì¼ í•˜ë‚˜í•˜ë‚˜ê°€ ê´€ê±´ì¼ ê±° ê°™ì•„ìš”!"
        )
    elif score >= 40:
        label = "ì¹œí•œ ì¹œêµ¬ ëŠë‚Œ ğŸ˜Š"
        topic = "í¸í•˜ê³  ì¬ë°ŒëŠ” ì¹œêµ¬ ëŠë‚Œì˜ ëŒ€í™”ê°€ ë§ì•„ìš”."
        summary = (
            "ì¼ìƒ ëŒ€í™”ì™€ ê°€ë²¼ìš´ ë†ë‹´ ìœ„ì£¼ë¼ ë¶„ìœ„ê¸°ëŠ” ì¢‹ì§€ë§Œ, ì•„ì§ ëšœë ·í•œ ì—°ì•  ë‰˜ì•™ìŠ¤ëŠ” ì ì–´ìš”. "
            "ì¡°ê¸ˆ ë” ì†”ì§í•œ í‘œí˜„ì´ë‚˜ ê°œì¸ì ì¸ ì´ì•¼ê¸°ë“¤ì„ ë˜ì ¸ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        )
    else:
        label = "ì—°ì•  ê°ì •ì€ ë‚®ì€ í¸ ğŸ˜¶"
        topic = "ì•„ì§ì€ ê´€ê³„ë¥¼ ì§€ì¼œë³´ëŠ” ë‹¨ê³„ì²˜ëŸ¼ ë³´ì—¬ìš”."
        summary = (
            "ëŒ€í™”ì—ì„œ ê°ì • í‘œí˜„ì´ ì ê±°ë‚˜, ê±°ì ˆÂ·íšŒí”¼ ëŠë‚Œì˜ í‘œí˜„ì´ ì¡°ê¸ˆ ì„ì—¬ ìˆì„ ìˆ˜ ìˆì–´ìš”. "
            "ë„ˆë¬´ ì¡°ê¸‰í•´í•˜ì§€ ë§ê³ , ìƒëŒ€ì˜ ìƒí™©ê³¼ ì»¨ë””ì…˜ì„ ë°°ë ¤í•˜ë©´ì„œ ì²œì²œíˆ ë‹¤ê°€ê°€ ë³´ì„¸ìš”."
        )

    return {
        "interestScore": int(score),
        "interestLabel": label,
        "topic": topic,
        "summary": summary,
    }


def infer_topic(keywords):
    info = infer_love_insight(keywords)
    return info["topic"]
